# -*- coding: utf-8 -*-
"""Metrics'_calculations_copy

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1khpI5RPb1LvOeIwmyqWfRna1rlCWMFfD
"""

# Connecting google drive to google colaboratory

from google.colab import drive
drive.mount('/content/drive')

"""# **0.0 Imports and database connection**"""

# Commented out IPython magic to ensure Python compatibility.
# IMPORTS & DB CONNECTION:

import pandas as pd
import sqlite3
import sqlalchemy
import networkx as nx
from statistics import mean
import random
import logging
import timeit
import itertools
import functools
import scipy.sparse
import scipy.sparse.csgraph
from random import sample
from collections import namedtuple
from collections import Counter
from itertools import chain, combinations
from networkx.generators import *
from networkx.exception import NetworkXError
from networkx.utils import not_implemented_for
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
plt.style.use('ggplot')
# %matplotlib inline
conn = sqlite3.connect('/content/drive/MyDrive/Thesis/colabtest1/NetworksRDB2.db')
cur = conn.cursor()

"""# **0.1 Getting graphs and subgraphs from database**"""

#### GETTING GRAPHS FROM DB. ####

# GRAPH_a:
GRAPH_a = nx.Graph()

nodes_GRAPH_a = "SELECT id FROM NODES_GRAPH_a ;"
cur.execute(nodes_GRAPH_a)
out1 = cur.fetchall()
GRAPH_a.add_nodes_from(out1[0])

edges_GRAPH_a = "SELECT id_1, id_2 FROM GitHub_Social_Network_GRAPH_a ;"
cur.execute(edges_GRAPH_a)
out2 = cur.fetchall()
GRAPH_a.add_edges_from(out2)

# GRAPH_b:
GRAPH_b = nx.Graph()

nodes_GRAPH_b = "SELECT unq_profile_id FROM NODES_GRAPH_b ;"
cur.execute(nodes_GRAPH_b)
out3 = cur.fetchall()
GRAPH_b.add_nodes_from(out3[0])

edges_GRAPH_b = "SELECT profile_id, has_friend_profile_id FROM Social_Circles_from_Facebook_GRAPH_b ;"
cur.execute(edges_GRAPH_b)
out4 = cur.fetchall()
GRAPH_b.add_edges_from(out4)

# GRAPH_c:
GRAPH_c = nx.DiGraph()

nodes_GRAPH_c = "SELECT also_bought_this_product_id_NUMBER_OF_NODES FROM NODES_GRAPH_c ;"
cur.execute(nodes_GRAPH_c)
out5 = cur.fetchall()
GRAPH_c.add_nodes_from(out5[0])

edges_GRAPH_c = "SELECT from_node_id, to_node_id FROM Amazon_Product_Co_Purchasing_Network_GRAPH_c ;"
cur.execute(edges_GRAPH_c)
out6 = cur.fetchall()
GRAPH_c.add_edges_from(out6)

# GRAPH_d:
GRAPH_d = nx.DiGraph()

nodes_GRAPH_d = "SELECT unq_edited_talk_page_user_id_NUMBER_OF_NODES FROM NODES_GRAPH_d ;"
cur.execute(nodes_GRAPH_d)
out7 = cur.fetchall()
GRAPH_d.add_nodes_from(out7[0])

edges_GRAPH_d = "SELECT editor_user_id, edited_talk_page_user_id FROM Communication_Network_Wikipedia_GRAPH_d ;"
cur.execute(edges_GRAPH_d)
out8 = cur.fetchall()
GRAPH_d.add_edges_from(out8)

# Takes around 3 minutes

print(GRAPH_a)
print(GRAPH_b)
print(GRAPH_c)
print(GRAPH_d)

# Getting the subgraphs from db:

# Strongly Connected Directed subgraph of GRAPH_c:

subgraph_c = nx.DiGraph()

subgraph_c_nodes = "SELECT Nodes FROM nodes_of_strongly_connected_directed_subgraph_of_GRAPH_c ;"
cur.execute(subgraph_c_nodes)
out_sub_c = cur.fetchall()
subgraph_c.add_nodes_from(out_sub_c[0])

subgraph_c_edges = "SELECT from_node, to_node FROM edges_of_strongly_connected_directed_subgraph_of_GRAPH_c;"
cur.execute(subgraph_c_edges)
out_sub_c2 = cur.fetchall()
subgraph_c.add_edges_from(out_sub_c2)

# Strongly Connected Directed subgraph of GRAPH_d:

subgraph_d = nx.DiGraph()

subgraph_d_nodes = "SELECT nodes FROM nodes_of_strongly_connected_directed_subgraph_of_GRAPH_d;"
cur.execute(subgraph_d_nodes)
out_sub_d = cur.fetchall()
subgraph_d.add_nodes_from(out_sub_d[0])

subgraph_d_edges = "SELECT from_node, to_node FROM edges_of_strongly_connected_directed_subgraph_of_GRAPH_d;"
cur.execute(subgraph_d_edges)
out_sub_d2 = cur.fetchall()
subgraph_d.add_edges_from(out_sub_d2)

print(subgraph_c)
print(subgraph_d)

"""# **0.2 Fetching weakly-disconnected directed graphs, (GRAPH_c GRAPH_d), into strongly connected subgraphs functions**"""

#    Create strongly connected subgraph for directed GRAPH_c function:

def fetch_hugest_subgraph_c(GRAPH_c):
    Gcc_c = max(nx.strongly_connected_components(GRAPH_c), key=len)
    giantC_c = GRAPH_c.subgraph(Gcc_c)
    logging.info('Fetched Giant Subgraph_c')
    return giantC_c

fetch_subgraph_c = fetch_hugest_subgraph_c(GRAPH_c)

print(fetch_subgraph_c)

# Exporting data of strongly connected directed subgraph_c of GRAPH_c in order to add it into db

# Get nodes of subgraph_c into a csv:

df_sub_nodes_c = pd.DataFrame(fetch_subgraph_c.nodes, columns=['Nodes'])
df_sub_nodes_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_sub_nodes_c.csv', index=False)

# Get edges of subgraph_c into a csv:

df_sub_edges_c = pd.DataFrame(fetch_subgraph_c.edges, columns = ['from_node', 'to_node'])
df_sub_edges_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_sub_edges_c.csv', index=False)

#    Create strongly connected subgraph for directed GRAPH_d function:

def fetch_hugest_subgraph_d(GRAPH_d):
    Gcc_d = max(nx.strongly_connected_components(GRAPH_d), key=len)
    giantC_d = GRAPH_d.subgraph(Gcc_d)
    logging.info('Fetched Giant Subgraph_d')
    return giantC_d

fetch_subgraph_d = fetch_hugest_subgraph_d(GRAPH_d)

print(fetch_subgraph_d)

# Exporting data of strongly connected directed subgraph_d of GRAPH_d in order to add it into db

# Get nodes of subgraph_d into a csv:

df_sub_nodes_d = pd.DataFrame(fetch_subgraph_d.nodes, columns=['nodes'])
df_sub_nodes_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_sub_nodes_d.csv', index=False)

# Get edges of subgraph_d into a csv:

df_sub_edges_d = pd.DataFrame(fetch_subgraph_d.edges, columns = ['from_node', 'to_node'])
df_sub_edges_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_sub_edges_d.csv', index=False)

"""# **1.0 Average path length calculation functions for every graph**"""

# 1.0 Average Path Calculation
#      L =sum{s,t in V} / {d(s, t)}{n(n-1)}

#     Function for random sample to calculate avg path of GRAPH_a:

def write_nodes_number_and_shortest_paths_a(GRAPH_a, n_samples_a=10_000, output_path_a='/content/drive/MyDrive/Thesis/colabtest1/graph_info_output_a.txt'):
    with open(output_path_a, encoding='utf-8', mode='w+') as f:
        for component in nx.connected_components(GRAPH_a):
            component_a = GRAPH_a.subgraph(component)
            nodes_a = component_a.nodes()
            lengths_a = []
            for _ in range(n_samples_a):
                n1, n2 = random.choices(list(nodes_a), k=2)
                length_a = nx.shortest_path_length(component_a, source=n1, target=n2)
                lengths_a.append(length_a)
            f.write(f'Nodes num: {len(nodes_a)}, shortest path mean: {mean(lengths_a)} \n')
            return lengths_a

# 1. Average Path Calculation
#      L =sum{s,t in V} / {d(s, t)}{n(n-1)}

#    Function for random sample to calculate avg path of GRAPH_b:

def write_nodes_number_and_shortest_paths_b(GRAPH_b, n_samples_b=10_000, output_path_b='/content/drive/MyDrive/Thesis/colabtest1/graph_info_output_b.txt'):
    with open(output_path_b, encoding='utf-8', mode='w+') as f:
        for component in nx.connected_components(GRAPH_b):
            component_b = GRAPH_b.subgraph(component)
            nodes_b = component_b.nodes()
            lengths_b = []
            for _ in range(n_samples_b):
                n1, n2 = random.choices(list(nodes_b), k=2)
                length_b = nx.shortest_path_length(component_b, source=n1, target=n2)
                lengths_b.append(length_b)
            f.write(f'Nodes num: {len(nodes_b)}, shortest path mean: {mean(lengths_b)} \n')
            return lengths_b

# 1. Average Path Calculation
#      L =sum{s,t in V} / {d(s, t)}{n(n-1)}

#    Function for random sample to calculate avg path of GRAPH_c:

def avg_path_length(subgraph_c, n_samples_c=10_000, output_path_c='/content/drive/MyDrive/Thesis/colabtest1/graph_info_output_c.txt'):
    with open(output_path_c, encoding='utf-8', mode='w+') as f:
            for component_c in nx.strongly_connected_components(subgraph_c):
                component_c = subgraph_c.subgraph(component_c)
                nodes_c = component_c.nodes()
                lengths_c = []
            for _ in range(n_samples_c):
                n1, n2 = random.choices(list(nodes_c), k=2)
                length_c = nx.shortest_path_length(component_c, source=n1, target=n2)
                lengths_c.append(length_c)
            f.write(f'Nodes num: {len(nodes_c)}, shortest path mean: {mean(lengths_c)} \n')
            return lengths_c

# 1. Average Path Calculation
#      L =sum{s,t in V} / {d(s, t)}{n(n-1)}

#    Function for random sample to calculate avg path of GRAPH_d:

def write_nodes_number_and_shortest_paths_d(subgraph_d, n_samples_d=10_000, output_path_d='/content/drive/MyDrive/Thesis/colabtest1/graph_info_output_d.txt'):
    with open(output_path_d, encoding='utf-8', mode='w+') as f:
        for component in nx.strongly_connected_components(subgraph_d):
            component_d = subgraph_d.subgraph(component)
            nodes_d = component_d.nodes()
            lengths_d = []
            for _ in range(n_samples_d):
                n1, n2 = random.choices(list(nodes_d), k=2)
                length_d = nx.shortest_path_length(component_d, source=n1, target=n2)
                lengths_d.append(length_d)
            f.write(f'Nodes num: {len(nodes_d)}, shortest path mean: {mean(lengths_d)} \n')
            return lengths_d

avg_path_a = write_nodes_number_and_shortest_paths_a(GRAPH_a)

avg_path_b = write_nodes_number_and_shortest_paths_b(GRAPH_b)

avg_path_length_c = avg_path_length(subgraph_c, n_samples_c=10_000)

avg_path_d = write_nodes_number_and_shortest_paths_d(subgraph_d)

"""# **2.0 Density calculation**"""

# 2.0 Density Calculation (undirected graphs):
#     D = 2e / [ v * ( v - 1 ) ]

Density_GRAPH_a = nx.density(GRAPH_a)
Density_GRAPH_b = nx.density(GRAPH_b)

# Results:

print(round(Density_GRAPH_a, 5))
print(round(Density_GRAPH_b, 5))

# 2.1 Density Calculation (directed graphs):
#     D = e / [ v * ( v - 1 ) ]

Density_GRAPH_c = nx.density(GRAPH_c)
Density_GRAPH_d = nx.density(GRAPH_d)

# Results:

print(round(Density_GRAPH_c, 10))
print(round(Density_GRAPH_d, 12))

"""# **3.0 Degree calculation**"""

# 3.0 Degree Calculation (undirected graphs):

Degree_GRAPH_a = GRAPH_a.degree()
Degree_GRAPH_b = GRAPH_b.degree()

# Results:

# Degree_GRAPH_a:
df_degree_a = pd.DataFrame(Degree_GRAPH_a, columns=['nodes', 'degrees'])
# Degree_GRAPH_b:
df_degree_b = pd.DataFrame(Degree_GRAPH_b, columns=['nodes', 'degrees'])

df_degree_a

df_degree_b

# 3.1 Degree Calculation (directed graphs):

Degree_GRAPH_c = GRAPH_c.degree()
Degree_GRAPH_d = GRAPH_d.degree()

# Results:

# Degree_GRAPH_c:
df_degree_c = pd.DataFrame(Degree_GRAPH_c, columns=['nodes', 'degrees'])
# Degree_GRAPH_d:
df_degree_d = pd.DataFrame(Degree_GRAPH_d, columns=['nodes', 'degrees'])

df_degree_c

df_degree_d

# Exporting 3 results

df_degree_a.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_a.csv', index=False)
df_degree_b.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_b.csv', index=False)
df_degree_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_c.csv', index=False)
df_degree_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_d.csv', index=False)

"""# **3.2 Average degree calculation**"""

# 3.2 Average Degree Calculation:

# GRAPH_a:
avg_degree_GRAPH_a = df_degree_a['degrees'].mean()
# GRAPH_b:
avg_degree_GRAPH_b = df_degree_b['degrees'].mean()
# GRAPH_c:
avg_degree_GRAPH_c = df_degree_c['degrees'].mean()
# GRAPH_d:
avg_degree_GRAPH_d = df_degree_d['degrees'].mean()

round(avg_degree_GRAPH_a, 3)

round(avg_degree_GRAPH_b, 3)

round(avg_degree_GRAPH_c, 3)

round(avg_degree_GRAPH_d, 3)

"""# **4.0 Diameter calculation**"""

#  4. Diameter approximation:
#     Diameter = maximum Eccentricity

diam_a = nx.approximation.diameter(GRAPH_a)
diam_b = nx.approximation.diameter(GRAPH_b)
diam_c = nx.approximation.diameter(subgraph_c)
diam_d = nx.approximation.diameter(subgraph_d)

print(diam_a)
print(diam_b)
print(diam_c)
print(diam_d)

"""# **5.0 Local clustering coefficient calculation**"""

# 5.0 Local Clustering Coefficient for nodes calculation:
#     for unweighted graphs:
#     c_u = \frac{2 T(u)}{deg(u)(deg(u)-1)}

clust_coef_a = nx.clustering(GRAPH_a, weight=None)
df_clust_coef_a = pd.DataFrame(clust_coef_a.items(), columns=['node','clustering_coefficient'])

clust_coef_b = nx.clustering(GRAPH_b, weight=None)
df_clust_coef_b = pd.DataFrame(clust_coef_b.items(), columns=['node','clustering_coefficient'])

clust_coef_c = nx.clustering(GRAPH_c, weight=None)
df_clust_coef_c = pd.DataFrame(clust_coef_c.items(), columns=['node','clustering_coefficient'])

clust_coef_d = nx.clustering(GRAPH_d, weight=None)
df_clust_coef_d = pd.DataFrame(clust_coef_d.items(), columns=['node','clustering_coefficient'])

# Took 40 mimutes (mostly because of GRAPH_d calculation)

df_clust_coef_a

df_clust_coef_d

# Exporting 5.0 Results

df_clust_coef_a.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_a.csv', index=False)
df_clust_coef_b.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_b.csv', index=False)
df_clust_coef_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_c.csv', index=False)
df_clust_coef_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_d.csv', index=False)

# 5.1 Average clustering coefficient calculation:

p_a1 = "/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_a.csv"
dfcc_a = pd.read_csv(p_a1)
average_clustering_coefficient_a = dfcc_a['clustering_coefficient'].mean()

p_b1 = "/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_b.csv"
dfcc_b = pd.read_csv(p_b1)
average_clustering_coefficient_b = dfcc_b['clustering_coefficient'].mean()

p_c1 = "/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_c.csv"
dfcc_c = pd.read_csv(p_c1)
average_clustering_coefficient_c = dfcc_c['clustering_coefficient'].mean()

p_d1 = "/content/drive/MyDrive/Thesis/colabtest1/df_clustering_coefficient_d.csv"
dfcc_d = pd.read_csv(p_d1)
average_clustering_coefficient_d = dfcc_d['clustering_coefficient'].mean()

average_clustering_coefficient_a

average_clustering_coefficient_b

average_clustering_coefficient_c

average_clustering_coefficient_d

"""# **6.0 Average clustering coefficient calculation**"""

# 6.0 Average Clustering Coefficient calculation:
#      C = \frac{1}{n}\sum_{v \in G} c_v

avg_cc_a = nx.average_clustering(GRAPH_a, weight=None)
avg_cc_b = nx.average_clustering(GRAPH_b, weight=None)
avg_cc_c = nx.average_clustering(GRAPH_c, weight=None)

print(round(avg_cc_a, 2))

print(round(avg_cc_b, 2))

print(round(avg_cc_c, 2))

avg_cl_co_d = nx.average_clustering(GRAPH_d, weight=None) # ----> took 40 minutes

print(round(avg_cl_co_d, 2))

avg_cc_d = nx.average_clustering(subgraph_d, weight=None)

print(round(avg_cc_d, 2))

"""# **7.0 Centrality measures calculation**"""

# 7.0  Centrality measures:

#  7.1.1  Degree Centrality calculation:
#         The degree centrality for a node v is the fraction of nodes it is connected to

degree_centr_a = nx.degree_centrality(GRAPH_a)
df_dc_a = pd.DataFrame(degree_centr_a.items(), columns=['node','degree_centrality'])

degree_centr_b = nx.degree_centrality(GRAPH_b)
df_dc_b = pd.DataFrame(degree_centr_b.items(), columns=['node','degree_centrality'])

degree_centr_c = nx.degree_centrality(GRAPH_c)
df_dc_c = pd.DataFrame(degree_centr_c.items(), columns=['node','degree_centrality'])

degree_centr_d = nx.degree_centrality(GRAPH_d)
df_dc_d = pd.DataFrame(degree_centr_d.items(), columns=['node','degree_centrality'])

# Exporting the above results as dataframes.csv:

df_dc_a.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_a.csv', index=False)
df_dc_b.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_b.csv', index=False)
df_dc_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_c.csv', index=False)
df_dc_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_d.csv', index=False)

# 7.1.2 Average degree centrality calculation

path_a1 = "/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_a.csv"
dfdc_a = pd.read_csv(path_a1)
average_degree_centrality_GRAPH_a = dfdc_a['degree_centrality'].mean()

path_b1 = "/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_b.csv"
dfdc_b = pd.read_csv(path_b1)
average_degree_centrality_GRAPH_b = dfdc_b['degree_centrality'].mean()

path_c1 = "/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_c.csv"
dfdc_c = pd.read_csv(path_c1)
average_degree_centrality_GRAPH_c = dfdc_c['degree_centrality'].mean()

path_d1 = "/content/drive/MyDrive/Thesis/colabtest1/df_degree_centrality_d.csv"
dfdc_d = pd.read_csv(path_d1)
average_degree_centrality_GRAPH_d = dfdc_d['degree_centrality'].mean()

print(average_degree_centrality_GRAPH_a)

print(average_degree_centrality_GRAPH_b)

print(average_degree_centrality_GRAPH_c)

print(average_degree_centrality_GRAPH_d)

# 7.2 Closeness Centrality calculation:
#     C(u) = \frac{n - 1}{\sum_{v=1}^{n-1} d(v, u)}

# Took 1 hour

cc_a = nx.closeness_centrality(GRAPH_a, wf_improved=True)

df_cc_a = pd.DataFrame(cc_a.items(), columns=['node','closeness_centrality'])
df_cc_a

# Took 53 minutes

cc_b = nx.closeness_centrality(GRAPH_b, wf_improved=True)

df_cc_b = pd.DataFrame(cc_b.items(), columns=['node','closeness_centrality'])
df_cc_b

# Calculating Closeness Centrality for big directed graphs (GRAPH_c, GRAPH_d):

def closeness_centrality(G, u=None, distance=None, wf_improved=True):
    if G.is_directed():
        G = G.reverse()  # create a reversed graph view

    if distance is not None:
        # use Dijkstra's algorithm with specified attribute as edge weight
        path_length = functools.partial(
            nx.single_source_dijkstra_path_length, weight=distance
        )
    else:
        path_length = nx.single_source_shortest_path_length

    if u is None:
        nodes = random.sample(G.nodes, 10000)
    else:
        nodes = [u]
    closeness_dict = {}
    for n in nodes:
        sp = path_length(G, n)
        totsp = sum(sp.values())
        len_G = len(G)
        _closeness_centrality = 0.0
        if totsp > 0.0 and len_G > 1:
            _closeness_centrality = (len(sp) - 1.0) / totsp
            # normalize to number of nodes-1 in connected part
            if wf_improved:
                s = (len(sp) - 1.0) / (len_G - 1)
                _closeness_centrality *= s
        closeness_dict[n] = _closeness_centrality
    if u is not None:
        return closeness_dict[u]
    return closeness_dict

# Took 7 minutes for random sample of 1000

cc_c = closeness_centrality(GRAPH_c, u=None, distance=None, wf_improved=True)
df_cc_c = pd.DataFrame(cc_c.items(), columns=['random_c_node','closeness_centrality'])
df_cc_c



# Took around 1.5 hours for random sample of 10_000

cc_c2 = closeness_centrality(GRAPH_c, u=None, distance=None, wf_improved=True)
df_cc_c2 = pd.DataFrame(cc_c2.items(), columns=['random_c_node','closeness_centrality'])
df_cc_c2

# Took 7 minutes for random sample of 1000

cc_d = closeness_centrality(GRAPH_d, u=None, distance=None, wf_improved=True)
df_cc_d = pd.DataFrame(cc_d.items(), columns=['random_c_node','closeness_centrality'])
df_cc_d

# Took around 1 hour for random sample of 10_000

cc_d2 = closeness_centrality(GRAPH_d, u=None, distance=None, wf_improved=True)
df_cc_d2 = pd.DataFrame(cc_d2.items(), columns=['random_c_node','closeness_centrality'])
df_cc_d2

# Get average closeness centralities of graphs:

# GRAPH_a:

path1 = "/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_a.csv"
dfcc1 = pd.read_csv(path1)
average_closeness_centrality_GRAPH_a = dfcc1['closeness_centrality'].mean()

# GRAPH_b:

path2 = "/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_b.csv"
dfcc2 = pd.read_csv(path2)
average_closeness_centrality_GRAPH_b = dfcc2['closeness_centrality'].mean()

# GRAPH_c:

path3 = "/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_c2.csv"
dfcc3 = pd.read_csv(path3)
average_closeness_centrality_GRAPH_c = dfcc3['closeness_centrality'].mean()

# GRAPH_d:

path4 = "/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_d2.csv"
dfcc4 = pd.read_csv(path4)
average_closeness_centrality_GRAPH_d = dfcc4['closeness_centrality'].mean()

average_closeness_centrality_GRAPH_a

average_closeness_centrality_GRAPH_b

average_closeness_centrality_GRAPH_c

average_closeness_centrality_GRAPH_d

# Exporting 7.2 Results:

df_cc_a.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_a.csv', index=False)
df_cc_b.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_b.csv', index=False)
df_cc_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_c.csv', index=False)
df_cc_c2.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_c2.csv', index=False)
df_cc_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_d.csv', index=False)
df_cc_d2.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_closeness_centrality_d2.csv', index=False)

# 7.3.1 Betweenness Centrality calculation:
#       c_B(v) =\sum_{s,t \in V} \frac{\sigma(s, t|v)}{\sigma(s, t)}

# Took 13 minutes for k=100

bc_a2 = nx.betweenness_centrality(GRAPH_a, k=1000,  weight=None, normalized=True, endpoints=True )

df_bc_a2 = pd.DataFrame(bc_a2.items(), columns=['node', 'betweenness_centrality'])

# Took 1.5 minutes for k=1000

bc_b2 = nx.betweenness_centrality(GRAPH_b, k=1000, weight=None, normalized=True, endpoints=True)

df_bc_b2 = pd.DataFrame(bc_b2.items(), columns=['node', 'betweenness_centrality'])

# Took 5 minutes for k=100

bc_c2 = nx.betweenness_centrality(GRAPH_c, k=100, weight=None, normalized=True, endpoints=True)

df_bc_c2 = pd.DataFrame(bc_c2.items(), columns=['node', 'betweenness_centrality'])

# Took 21 minutes for k=15

bc_d2 = nx.betweenness_centrality(GRAPH_d, k=15, weight=None, normalized=True, endpoints=True)

df_bc_d2 = pd.DataFrame(bc_d2.items(), columns=['node', 'betweenness_centrality'])

# Exporting 7.3.1 Results

df_bc_a2.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_a.csv', index=False)
df_bc_b2.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_b.csv', index=False)
df_bc_c2.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_c.csv', index=False)
df_bc_d2.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_d.csv', index=False)

# 7.3.2 Average betweenness centrality calculation

patha_2 = "/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_a.csv"
dfbc_a2 = pd.read_csv(patha_2)
average_betweenness_centrality_a2 = dfbc_a2['betweenness_centrality'].mean()

pathb_2 = "/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_b.csv"
dfbc_b2 = pd.read_csv(pathb_2)
average_betweenness_centrality_b2 = dfbc_b2['bc'].mean()

pathc_2 = "/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_c.csv"
dfbc_c2 = pd.read_csv(pathc_2)
average_betweenness_centrality_c2 = dfbc_c2['betweenness_centrality'].mean()

pathd_2 = "/content/drive/MyDrive/Thesis/colabtest1/df_betweenness_centrality_GRAPH_d.csv"
dfbc_d2 = pd.read_csv(pathd_2)
average_betweenness_centrality_d2 = dfbc_d2['betweenness_centrality'].mean()

average_betweenness_centrality_a2

average_betweenness_centrality_b2

average_betweenness_centrality_c2

average_betweenness_centrality_d2

# 7.4.1 Eigenvector Centrality calculation:
#       Ax = \lambda x

eig_centr_a = nx.eigenvector_centrality_numpy(GRAPH_a, weight=None, max_iter=10_000)
df_eig_centr_a = pd.DataFrame(eig_centr_a.items(), columns=['node','eigenvector_centrality'])

eig_centr_b = nx.eigenvector_centrality_numpy(GRAPH_b, weight=None, max_iter=10_000)
df_eig_centr_b = pd.DataFrame(eig_centr_b.items(), columns=['node','eigenvector_centrality'])

eig_centr_c = nx.eigenvector_centrality_numpy(GRAPH_c, weight=None, max_iter=10_000, tol=1e-03)
df_eig_centr_c = pd.DataFrame(eig_centr_c.items(), columns=['node','eigenvector_centrality'])

eig_centr_d = nx.eigenvector_centrality_numpy(GRAPH_d, weight=None, max_iter=10_000)
df_eig_centr_d = pd.DataFrame(eig_centr_d.items(), columns=['node','eigenvector_centrality'])

# Exporting results of 7.4

df_eig_centr_a.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_a.csv', index=False)
df_eig_centr_b.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_b.csv', index=False)
df_eig_centr_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_c.csv', index=False)
df_eig_centr_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_d.csv', index=False)

# 7.4.2 Average eigenvector centrality calculation:

path_a_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_a.csv"
dfec_a = pd.read_csv(path_a_1)
average_eigenvector_centrality_a = dfec_a['eigenvector_centrality'].mean()

path_b_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_b.csv"
dfec_b = pd.read_csv(path_b_1)
average_eigenvector_centrality_b = dfec_b['eigenvector_centrality'].mean()

path_c_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_c.csv"
dfec_c = pd.read_csv(path_c_1)
average_eigenvector_centrality_c = dfec_c['eigenvector_centrality'].mean()

path_d_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_eigenvector_centrality_d.csv"
dfec_d = pd.read_csv(path_d_1)
average_eigenvector_centrality_d = dfec_d['eigenvector_centrality'].mean()

average_eigenvector_centrality_a

average_eigenvector_centrality_b

average_eigenvector_centrality_c

average_eigenvector_centrality_d

"""# **8.0 Number of triangles that include a node as one vertex calculation**"""

# 8.1 Number of triangles that include a node as one vertex calculation:
#      For undirected graphs (GRAPH_a, GRAPH_b)

triangles_a = nx.triangles(GRAPH_a)
df_triangles_a = pd.DataFrame(triangles_a.items(), columns=['node','number_of_triangles'])

triangles_b = nx.triangles(GRAPH_b)
df_triangles_b = pd.DataFrame(triangles_b.items(), columns=['node','number_of_triangles'])

df_triangles_a

df_triangles_b

# Function to calculate triangles for directed GRAPH_c:

def directed_triangles(GRAPH_c):
    nodes_c = GRAPH_c.nodes
    nodes_nbrs_c = ((n, GRAPH_c._pred[n], GRAPH_c._succ[n]) for n in GRAPH_c.nbunch_iter(nodes_c))

    for i, preds, succs in nodes_nbrs_c:
        ipreds_c = set(preds) - {i}
        isuccs_c = set(succs) - {i}

        directed_triangles_c = 0
        for j in chain(ipreds_c, isuccs_c):
            jpreds_c = set(GRAPH_c._pred[j]) - {j}
            jsuccs_c = set(GRAPH_c._succ[j]) - {j}
            directed_triangles_c += sum(
                1
                for k in chain(
                    (ipreds_c & jpreds_c),
                    (ipreds_c & jsuccs_c),
                    (isuccs_c & jpreds_c),
                    (isuccs_c & jsuccs_c),
                )
            )
        dtotal_c = len(ipreds_c) + len(isuccs_c)
        dbidirectional_c = len(ipreds_c & isuccs_c)
        yield (i, dtotal_c, dbidirectional_c, directed_triangles_c)

triangles_c = directed_triangles(GRAPH_c)

df_triangles_c = pd.DataFrame(list(iter(triangles_c)), columns=['node','total_degree','reciprocal_degree','number_of_directed_triangles'])
df_triangles_c

# Function to calculate triangles for directed GRAPH_d:

def directed_triangles(GRAPH_d):
    nodes_d = GRAPH_d.nodes
    nodes_nbrs_d = ((n, GRAPH_d._pred[n], GRAPH_d._succ[n]) for n in GRAPH_d.nbunch_iter(nodes_d))

    for i, preds, succs in nodes_nbrs_d:
        ipreds_d = set(preds) - {i}
        isuccs_d = set(succs) - {i}

        directed_triangles_d = 0
        for j in chain(ipreds_d, isuccs_d):
            jpreds_d = set(GRAPH_d._pred[j]) - {j}
            jsuccs_d = set(GRAPH_d._succ[j]) - {j}
            directed_triangles_d += sum(
                1
                for k in chain(
                    (ipreds_d & jpreds_d),
                    (ipreds_d & jsuccs_d),
                    (isuccs_d & jpreds_d),
                    (isuccs_d & jsuccs_d),
                )
            )
        dtotal_d = len(ipreds_d) + len(isuccs_d)
        dbidirectional_d = len(ipreds_d & isuccs_d)
        yield (i, dtotal_d, dbidirectional_d, directed_triangles_d)

triangles_d = directed_triangles(GRAPH_d)

# Took 38.31 minutes

df_triangles_d = pd.DataFrame(list(iter(triangles_d)), columns=['node','total_degree','reciprocal_degree','number_of_directed_triangles'])
df_triangles_d

# Exporting 8.0 Results

df_triangles_a.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_triangles_a.csv', index=False)
df_triangles_b.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_triangles_b.csv', index=False)
df_triangles_c.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_triangles_c.csv', index=False)
df_triangles_d.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_triangles_d.csv', index=False)

# 8.2.1 Average number of triangles calculation for undirected graphs (GRAPH_a, GRAPH_b):

p_a_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_triangles_a.csv"
dftr_a = pd.read_csv(p_a_1)
average_number_of_triangles_a = dftr_a['number_of_triangles'].mean()

p_b_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_triangles_b.csv"
dftr_b = pd.read_csv(p_b_1)
average_number_of_triangles_b = dftr_b['number_of_triangles'].mean()

average_number_of_triangles_a

average_number_of_triangles_b

# 8.2.2 Average total_degree,reciprocal_degree,number_of_directed_triangles for directed graphs (GRAPH_c, GRAPH_d) calculation:

p_c_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_triangles_c.csv"
dftr_c = pd.read_csv(p_c_1)
average_data_c = dftr_c[["total_degree","reciprocal_degree","number_of_directed_triangles"]].mean()

p_d_1 = "/content/drive/MyDrive/Thesis/colabtest1/df_triangles_d.csv"
dftr_d = pd.read_csv(p_d_1)
average_data_d = dftr_d[["total_degree","reciprocal_degree","number_of_directed_triangles"]].mean()

average_data_c

average_data_d

"""# **9.0 Transitivity calculation**"""

# 9.0 Transitivity Calculation:
#     The fraction of all possible triangles present in G.
#           T = 3\frac{\#triangles}{\#triads}

trans_a = nx.transitivity(GRAPH_a)
trans_b = nx.transitivity(GRAPH_b)
trans_c = nx.transitivity(GRAPH_c)
trans_d = nx.transitivity(GRAPH_d)

# Took 3.47 minutes

print(trans_a)
print(trans_b)
print(trans_c)
print(trans_d)

"""# **10.0 Local bridges for undirected graphs, (GRAPH_a, GRAPH_b), calculation**"""

# 10.0 Local Bridges
#      for undirected graphs (GRAPH_a, GRAPH_b)

local_bridges_a = nx.local_bridges(GRAPH_a, with_span=False, weight=None)
df_local_bridges_a = pd.DataFrame(local_bridges_a, columns=['node','local_bridge'])

local_bridges_b = nx.local_bridges(GRAPH_b, with_span=False, weight=None)
df_local_bridges_b = pd.DataFrame(local_bridges_b, columns=['node','local_bridge'])

df_local_bridges_a

df_local_bridges_b

# Exporting 10.0 results

df_local_bridges_a.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_local_bridges_a.csv', index=False)
df_local_bridges_b.to_csv(r'/content/drive/MyDrive/Thesis/colabtest1/df_local_bridges_b.csv', index=False)

"""#  **NOTES**"""

# array = np.array(Degree_GRAPH_d)

# array.shape

# array[:,1].mean()

# np.mean(array[:,1])

# df1 = pd.DataFrame(Degree_GRAPH_d, columns=['nodes', 'degrees'])

# df1.head()

# df1["degrees"].mean()

# df1["degrees"].values

# array[:,1]

# np.unique(array[:,1], return_counts=True)

# size = np.unique(array[:,1], return_counts=True)[1].size

# plt.bar(np.arange(size), np.unique(array[:,1], return_counts=True)[1], width=1.5)
# plt.ylim(0, 100)

# np.unique(array[:,1], return_counts=True)[1]

# size

# print(nx.is_strongly_connected(GRAPH_c)) # ---> False  ---> make a strongly_conn subgraph

# print(nx.is_strongly_connected(GRAPH_d)) # ---> True



"""# **Visualizing process**"""

